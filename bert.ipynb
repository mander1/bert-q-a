{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c604a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    BertConfig,\n",
    "    BertForMaskedLM,\n",
    "    BertTokenizerFast,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Resource monitoring function\n",
    "def print_resource_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / (1024 * 1024)\n",
    "    cpu_percent = psutil.cpu_percent(interval=0.1)\n",
    "    print(f\"Memory usage: {memory_mb:.2f} MB | CPU usage: {cpu_percent:.1f}%\")\n",
    "\n",
    "# Setting device - using CPU\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CPU: {psutil.cpu_count(logical=True)} logical cores\")\n",
    "print(f\"RAM: {psutil.virtual_memory().total / (1024**3):.1f} GB total\")\n",
    "print_resource_usage()\n",
    "\n",
    "# Configure to use less memory\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load a tiny dataset (SQuAD is good for QA)\n",
    "# Using a very small subset for rapid pretraining (~30min)\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"squad\", split=\"train[:500]\")  # Just 500 examples for faster training\n",
    "print(f\"Dataset loaded with {len(dataset)} examples\")\n",
    "print_resource_usage()\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Tokenize the dataset\n",
    "max_length = 64  # Using very small sequence length to speed up training\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    # Combine the question and context/answer for MLM pretraining\n",
    "    texts = [\n",
    "        f\"Question: {q} Context: {c}\" \n",
    "        for q, c in zip(examples[\"question\"], examples[\"context\"])\n",
    "    ]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "print(\"Tokenizing dataset...\")\n",
    "start_time = time.time()\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=32,\n",
    "    num_proc=2,  # Using 2 processes on your i7 is fine\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "print(f\"Tokenization completed in {time.time() - start_time:.2f} seconds\")\n",
    "print_resource_usage()\n",
    "\n",
    "# Data collator for masked language modeling\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "# Configure a tiny BERT model for pretraining (for ~30min runtime)\n",
    "smaller_config = BertConfig(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    hidden_size=128,      # Tiny hidden size (default is 768)\n",
    "    num_hidden_layers=3,  # Minimal layers (default is 12)\n",
    "    num_attention_heads=2,  # Minimal attention heads (default is 12)\n",
    "    intermediate_size=256,  # Tiny intermediate size (default is 3072)\n",
    "    max_position_embeddings=128  # Match our sequence length\n",
    ")\n",
    "\n",
    "# Initialize model with smaller config to reduce memory usage\n",
    "model = BertForMaskedLM(config=smaller_config)\n",
    "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())/1e6:.2f}M parameters\")\n",
    "\n",
    "# Set up training arguments optimized for ~30min runtime on quad-core i7\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_pretrained_qa\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,  # More epochs since we have a tiny model and dataset\n",
    "    per_device_train_batch_size=16,  # Larger batch size is fine with your 16GB RAM\n",
    "    save_steps=250,\n",
    "    save_total_limit=1,  # Only keep the best model to save disk space\n",
    "    prediction_loss_only=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    # Optimization for speed\n",
    "    fp16=False,\n",
    "    dataloader_num_workers=2,  # Use 2 workers on your i7\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard\n",
    "    # Additional speed optimizations\n",
    "    gradient_accumulation_steps=1,\n",
    "    warmup_steps=50,\n",
    "    weight_decay=0.01,\n",
    "    # Performance monitoring\n",
    "    evaluation_strategy=\"no\",\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model with timing\n",
    "print(\"Starting training...\")\n",
    "print_resource_usage()\n",
    "start_time = time.time()\n",
    "\n",
    "# Create a custom callback to monitor time and resources\n",
    "class ResourceMonitorCallback(TrainingArguments):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if state.is_local_process_zero:\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f\"Step: {state.global_step} | Time elapsed: {elapsed:.2f}s | Est. remaining: {(elapsed/state.global_step)*(state.max_steps-state.global_step):.2f}s\")\n",
    "            print_resource_usage()\n",
    "\n",
    "trainer.add_callback(ResourceMonitorCallback())\n",
    "trainer.train()\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"Training completed in {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
    "print_resource_usage()\n",
    "\n",
    "# Save the pretrained model\n",
    "model_path = \"./bert_pretrained_qa_final\"\n",
    "trainer.save_model(model_path)\n",
    "tokenizer.save_pretrained(model_path)\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Optional: Fine-tune for QA task after pretraining\n",
    "print(\"Pretraining complete! You can now fine-tune this model for your specific QA task.\")\n",
    "print(\"Example fine-tuning code:\")\n",
    "print(\"\"\"\n",
    "from transformers import BertForQuestionAnswering, Trainer, TrainingArguments\n",
    "\n",
    "# Load pretrained model\n",
    "model = BertForQuestionAnswering.from_pretrained(\"./bert_pretrained_qa_final\")\n",
    "\n",
    "# Process SQuAD data for QA (with start/end positions)\n",
    "# ... data processing code ...\n",
    "\n",
    "# Set up QA training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_qa_finetuned\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    # Add other training arguments\n",
    ")\n",
    "\n",
    "# Initialize trainer for QA\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    # Add validation dataset and metrics\n",
    ")\n",
    "\n",
    "# Fine-tune for QA\n",
    "trainer.train()\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertie",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
